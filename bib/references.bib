


@article{PENG2020364,
title = {Wild animal survey using UAS imagery and deep learning: modified Faster R-CNN for kiang detection in Tibetan Plateau},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {169},
pages = {364-376},
year = {2020},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2020.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0924271620302409},
author = {Jinbang Peng and Dongliang Wang and Xiaohan Liao and Quanqin Shao and Zhigang Sun and Huanyin Yue and Huping Ye},
keywords = {Wild animal survey, Deep learning, Object detection, Unmanned aircraft systems (UAS)},
abstract = {Wild animal surveys play a critical role in wild animal conservation and ecosystem management. Unmanned aircraft systems (UASs), with advantages in safety, convenience and inexpensiveness, have been increasingly used in wild animal surveys. However, manually reviewing wild animals from thousands of images generated by UASs is tedious and inefficient. To support wild animal detection in UAS images, researchers have developed various automatic and semiautomatic algorithms. Among these algorithms, deep learning techniques achieve outstanding performances in wild animal detection, but have some practical issues (e.g., limited animal pixels and sparse animal samples). Based on a typical deep learning pipeline, faster region based convolutional neural networks (Faster R-CNN), this study adopted several tactics, including feature stride shortening, anchor size optimization, and hard negative class, to overcome the practical issues in wild animal detection in UAS images. In this study, a kiang survey was conducted in UAS datasets (23,748 images) obtained by 14 flight campaigns in the eastern Tibetan Plateau. The validation experiments of our adopted tactics revealed the following: (1) feature stride shortening and anchor size optimization improved small animal detection performance in the animal patch set, increasing the F1 score from 0.84 to 0.86 and from 0.86 to 0.92, respectively; and (2) the hard negative class significantly suppressed false positives in the full UAS image set, increasing the F1 score from 0.44 to 0.86. The test results in the full UAS image set showed that the modified model with the adopted tactics can be applied to either a semiautomatic survey to accelerate manual verification by 25 times or an automatic survey with an F1 score of approximately 0.90. This study demonstrates that the combination of UAS and deep learning techniques can enable automatic/semiautomatic, accurate, inexpensive, and efficient wild animal surveys.}
}



@Article{ai2040034,
AUTHOR = {Ibraheam, Mai and Li, Kin Fun and Gebali, Fayez and Sielecki, Leonard E.},
TITLE = {A Performance Comparison and Enhancement of Animal Species Detection in Images with Various R-CNN Models},
JOURNAL = {AI},
VOLUME = {2},
YEAR = {2021},
NUMBER = {4},
PAGES = {552--577},
URL = {https://www.mdpi.com/2673-2688/2/4/34},
ISSN = {2673-2688},
ABSTRACT = {Object detection is one of the vital and challenging tasks of computer vision. It supports a wide range of applications in real life, such as surveillance, shipping, and medical diagnostics. Object detection techniques aim to detect objects of certain target classes in a given image and assign each object to a corresponding class label. These techniques proceed differently in network architecture, training strategy and optimization function. In this paper, we focus on animal species detection as an initial step to mitigate the negative impacts of wildlifeâ€“human and wildlifeâ€“vehicle encounters in remote wilderness regions and on highways. Our goal is to provide a summary of object detection techniques based on R-CNN models, and to enhance the performance of detecting animal species in accuracy and speed, by using four different R-CNN models and a deformable convolutional neural network. Each model is applied on three wildlife datasets, results are compared and analyzed by using four evaluation metrics. Based on the evaluation, an animal species detection system is proposed.},
DOI = {10.3390/ai2040034}
}



@article{XU2024103732,
title = {A review of deep learning techniques for detecting animals in aerial and satellite images},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {128},
pages = {103732},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.103732},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224000864},
author = {Zeyu Xu and Tiejun Wang and Andrew K. Skidmore and Richard Lamprey},
keywords = {Biodiversity, Wildlife, Livestock, Remote sensing, Artificial intelligence, Object detection},
abstract = {Deep learning is an effective machine learning method that in recent years has been successfully applied to detect and monitor species population in remotely sensed data. This study aims to provide a systematic literature review of current applications of deep learning methods for animal detection in aerial and satellite images. We categorized methods in collated publications into image level, point level, bounding-box level, instance segmentation level, and specific information level. The statistical results show that YOLO, Faster R-CNN, U-Net and ResNet are the most used neural network structures. The main challenges associated with the use of these deep learning methods are imbalanced datasets, small samples, small objects, image annotation methods, image background, animal counting, model accuracy assessment, and uncertainty estimation. We explored possible solutions include the selection of sample annotation methods, optimizing positive or negative samples, using weakly and self-supervised learning methods, selecting or developing more suitable network structures. Future research trends we identified are video-based detection, very high-resolution satellite image-based detection, multiple species detection, new annotation methods, and the development of specialized network structures and large foundation models. We discussed existing research attempts as well as personal perspectives on these possible solutions and future trends.}
}


@article{10.1007/s11042-021-11290-4,
author = {Chandrakar, Ramakant and Raja, Rohit and Miri, Rohit},
title = {Animal detection based on deep convolutional neural networks with genetic segmentation},
year = {2022},
issue_date = {Dec 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {81},
number = {29},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-11290-4},
doi = {10.1007/s11042-021-11290-4},
abstract = {This paper presents a system for automatic detection and recognition of the animals using Deep CNN with genetic segmentation. In the present work, the grouping of input animal pictures is done with the help of a Convolutional Neural Network is demonstrated. The proposed work is compared with standard recognition methods such as SU, DS, MDF, LEGS, DRFI, MR, GC. The existing methodologies have more error rates because of high false-positive \& negative rate detection, hence there is a need for a highly accurate system for animal detection. According to the proposed work, a genetic algorithm is used for the segmentation process, and for classification 3-layers neural network is used. For training and examining the proposed work, a database is created which consists of 100 distinct subjects with 2 classes and 10 pictures in each class. Experimental results are demonstrated as the segmentation using genetic algorithms and the novelty of the proposed method in terms of precision, recall, f-measurement, and MAE. Hence proposed work improves the overall results i.e. precision (99.02\%), recall (98.79\%), F-Measurement (98.9\%), and MAE (0.78\%).},
journal = {Multimedia Tools Appl.},
month = {dec},
pages = {42149â€“42162},
numpages = {14},
keywords = {Animal classification, CNN, Saliency map, Genetic algorithm}
}



@Article{ani13091526,
AUTHOR = {Binta Islam, Sazida and Valles, Damian and Hibbitts, Toby J. and Ryberg, Wade A. and Walkup, Danielle K. and Forstner, Michael R. J.},
TITLE = {Animal Species Recognition with Deep Convolutional Neural Networks from Ecological Camera Trap Images},
JOURNAL = {Animals},
VOLUME = {13},
YEAR = {2023},
NUMBER = {9},
ARTICLE-NUMBER = {1526},
URL = {https://www.mdpi.com/2076-2615/13/9/1526},
PubMedID = {37174563},
ISSN = {2076-2615},
ABSTRACT = {Accurate identification of animal species is necessary to understand biodiversity richness, monitor endangered species, and study the impact of climate change on species distribution within a specific region. Camera traps represent a passive monitoring technique that generates millions of ecological images. The vast numbers of images drive automated ecological analysis as essential, given that manual assessment of large datasets is laborious, time-consuming, and expensive. Deep learning networks have been advanced in the last few years to solve object and species identification tasks in the computer vision domain, providing state-of-the-art results. In our work, we trained and tested machine learning models to classify three animal groups (snakes, lizards, and toads) from camera trap images. We experimented with two pretrained models, VGG16 and ResNet50, and a self-trained convolutional neural network (CNN-1) with varying CNN layers and augmentation parameters. For multiclassification, CNN-1 achieved 72% accuracy, whereas VGG16 reached 87%, and ResNet50 attained 86% accuracy. These results demonstrate that the transfer learning approach outperforms the self-trained model performance. The models showed promising results in identifying species, especially those with challenging body sizes and vegetation.},
DOI = {10.3390/ani13091526}
}




@Article{drones7030179,
AUTHOR = {RanÄiÄ‡, Kristina and BlagojeviÄ‡, BoÅ¡ko and Bezdan, Atila and IvoÅ¡eviÄ‡, Bojana and TubiÄ‡, Bojan and VraneÅ¡eviÄ‡, Milica and Pejak, Branislav and CrnojeviÄ‡, Vladimir and Marko, Oskar},
TITLE = {Animal Detection and Counting from UAV Images Using Convolutional Neural Networks},
JOURNAL = {Drones},
VOLUME = {7},
YEAR = {2023},
NUMBER = {3},
ARTICLE-NUMBER = {179},
URL = {https://www.mdpi.com/2504-446X/7/3/179},
ISSN = {2504-446X},
ABSTRACT = {In the last decade, small unmanned aerial vehicles (UAVs/drones) have become increasingly popular in the airborne observation of large areas for many purposes, such as the monitoring of agricultural areas, the tracking of wild animals in their natural habitats, and the counting of livestock. Coupled with deep learning, they allow for automatic image processing and recognition. The aim of this work was to detect and count the deer population in northwestern Serbia from such images using deep neural networks, a tedious process that otherwise requires a lot of time and effort. In this paper, we present and compare the performance of several state-of-the-art network architectures, trained on a manually annotated set of images, and use it to predict the presence of objects in the rest of the dataset. We implemented three versions of the You Only Look Once (YOLO) architecture and a Single Shot Multibox Detector (SSD) to detect deer in a dense forest environment and measured their performance based on mean average precision (mAP), precision, recall, and F1 score. Moreover, we also evaluated the models based on their real-time performance. The results showed that the selected models were able to detect deer with a mean average precision of up to 70.45% and a confidence score of up to a 99%. The highest precision was achieved by the fourth version of YOLO with 86%, as well as the highest recall value of 75%. Its compressed version achieved slightly lower results, with 83% mAP in its best case, but it demonstrated four times better real-time performance. The counting function was applied on the best-performing models, providing us with the exact distribution of deer over all images. Yolov4 obtained an error of 8.3% in counting, while Yolov4-tiny mistook 12 deer, which accounted for an error of 7.1%.},
DOI = {10.3390/drones7030179}
}



@article{article,
author = {Pal, Sankar and Pramanik, Anima and Maiti, Jhareswar and Mitra, Pabitra},
year = {2021},
month = {09},
pages = {},
title = {Deep learning in multi-object detection and tracking: state of the art},
volume = {51},
journal = {Applied Intelligence},
doi = {10.1007/s10489-021-02293-7}
}


@INPROCEEDINGS{9774826,
  author={Palanisamy, Vigneshwaran and Ratnarajah, Nagulan},
  booktitle={2021 21st International Conference on Advances in ICT for Emerging Regions (ICter)},
  title={Detection of Wildlife Animals using Deep Learning Approaches: A Systematic Review},
  year={2021},
  volume={},
  number={},
  pages={153-158},
  keywords={Deep learning;Image segmentation;Image recognition;Systematics;Wildlife;Sociology;Ecosystems;Wildlife Animals;Detection;Ecosystem;Camera trap images;Deep learning;Review},
  doi={10.1109/ICter53630.2021.9774826}}


@article{Alzubaidi2021ReviewOD,
  title={Review of deep learning: concepts, CNN architectures, challenges, applications, future directions},
  author={Laith Alzubaidi and Jinglan Zhang and Amjad J. Humaidi and Ayad Al-dujaili and Ye Duan and Omran Al-Shamma and Jos{\'e} I. Santamar{\'i}a and Mohammed Abdulraheem Fadhel and Muthana Al-Amidie and Laith Farhan},
  journal={Journal of Big Data},
  year={2021},
  volume={8},
  url={https://api.semanticscholar.org/CorpusID:232434552}
}





@inproceedings{10.1145/3613904.3642115,
author = {Schneiders, Eike and Benford, Steve and Chamberlain, Alan and Mancini, Clara and Castle-Green, Simon and Ngo, Victor and Row Farr, Ju and Adams, Matt and Tandavanitj, Nick and Fischer, Joel},
title = {Designing Multispecies Worlds for Robots, Cats, and Humans},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642115},
doi = {10.1145/3613904.3642115},
abstract = {We reflect on the design of a multispecies world centred around a bespoke enclosure in which three cats and a robot arm coexist for six hours a day during a twelve-day installation as part of an artist-led project. In this paper, we present the projectâ€™s design process, encompassing various interconnected components, including the cats, the robot and its autonomous systems, the custom end-effectors and robot attachments, the diverse roles of the humans-in-the-loop, and the custom-designed enclosure. Subsequently, we provide a detailed account of key moments during the deployment and discuss the design implications for future multispecies systems. Specifically, we argue that designing the technology and its interactions is not sufficient, but that it is equally important to consider the design of the â€˜worldâ€™ in which the technology operates. Finally, we highlight the necessity of human involvement in areas such as breakdown recovery, animal welfare, and their role as audience.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {593},
numpages = {16},
keywords = {animal-computer interaction, artist-led research, performance-led research},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI '24}
}

